{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcd320d9",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# RAG with langchain\n",
    "\n",
    "The purpose of this exercise is to show how using a tool like LangChain is different from the bare-bones implementation of the previous tutorial and the potential benefits of such a tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a68f7b",
   "metadata": {
    "cell_marker": "\"\"\"",
    "jp-MarkdownHeadingCollapsed": true,
    "lines_to_next_cell": 0
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "For this notebook we need two more Python modules. Again, my suggestion is to open a command line interface **in Jupyter** (`File -> New... -> Terminal`) and run the commands there instead of in this notebook:\n",
    "\n",
    "```\n",
    "jovyan@jupyter-user:~$ pip install langchain langchain_community\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f296af",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Baseline\n",
    "\n",
    "Before getting into RAG with LangChain, let's again establish a baseline by querying our LLM without using RAG. Note that LangChain is essentially a programming environment that wraps all of our other tools, for good or bad. So, instead of importing the client API using `from ollama import Client` as in the previous part, we get a module from LangChain's communityâ€¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763a0999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "ollama_host = 'http://10.129.20.4:9090'\n",
    "ollama_model = 'llama3:70b'\n",
    "\n",
    "# Simple chain invocation\n",
    "llm = Ollama(model=ollama_model, base_url=ollama_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ef731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an AI assistant. Your task is to understand the user question, and provide an answer.\n",
    "\n",
    "            Your answers are short, to the point, and written by an domain expert.\n",
    "            If you don't know the answer, simply state, \"I don't know\"\n",
    "            \"\"\"\n",
    "        ),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658dec77",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Here comes the part from which LangChain takes its name: parts of the application are \"chained\" together using a syntax reminding of UNIX pipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd4ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "query = \"What is special about HackerNews?\"\n",
    "response = chain.invoke({\"user_input\": query})\n",
    "print(response)\n",
    "\n",
    "# Verify that the answer is \"I don't know\"\n",
    "# query = \"What do you know about mr. Mjptkck?\"\n",
    "# response = chain.invoke({\"user_input\": query})\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc66baa",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Most tutorials add an output parser at the end of the chain, but in this case it is simply a passthrough, adding nothing of value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5ff206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output = StrOutputParser() # Basically a NOP is this example\n",
    "\n",
    "chain = prompt | llm | output\n",
    "\n",
    "query = \"What is special about HackerNews?\"\n",
    "response = chain.invoke({\"user_input\": query})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3b186f",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Chunking\n",
    "\n",
    "For the sake of comparison, we will use the plain python code example from the previous part to split the data into senteces (chunks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d2716b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Split the input data into sentence-sized chunks\n",
    "#\n",
    "import re\n",
    "import json\n",
    "\n",
    "chunks = []\n",
    "index = 0\n",
    "\n",
    "filenames = [\"newsfaq.json\", \"newsguidelines.json\", \"security.json\", \"legal.json\"]\n",
    "# Iterate over the entries in data/ and read each JSON file in turn\n",
    "for filename in filenames:\n",
    "    filepath = f\"./data/{filename}\"\n",
    "    with open(filepath) as fd:\n",
    "        data = json.load(fd)\n",
    "        \n",
    "    url = data['url']\n",
    "    text = data['content']\n",
    "    # Split the file's text contents into sentences using python regex:\n",
    "    #   A sequence of characters is deemed a sentence if followed by a\n",
    "    #   full stop (.), question mark (?), or an exclamation mark (!)\n",
    "    #   immediately followed by one or more whitespaces.\n",
    "    sentences = re.split(r\"(?<=\\.|\\?|!)\\s+\", text)\n",
    "    # Each sentence make up a chunk, store it with references (url and id)\n",
    "    for sentence in sentences:\n",
    "        chunks.append({'id': index, 'text': sentence, 'url': url})\n",
    "        index += 1\n",
    "\n",
    "# Write the resulting array to file:\n",
    "with open('chunks.json', 'w') as fd:\n",
    "    json.dump(chunks, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c946a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a sanity check, it should be ~570 chunks\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90febd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the sentences from our chunks\n",
    "sentences = [chunk['text'] for chunk in chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5d44f5",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Embedding and retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5050604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a named _collection_ making up our corner of the database (it is a shared resource)\n",
    "collection_name = \"<unique name here>\"\n",
    "collection_name = \"lc_hackernews\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be35a4d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "This is where LangChain IMHO gets a wee bit ugly as we must load yet another\n",
    "module tied to the implementation (_leaky abstraction_) and we must install `langchain-huggingface`.\n",
    "\n",
    "```\n",
    "jovyan@jupyter-user:~$ pip install langchain-huggingface\n",
    "```\n",
    "\n",
    "Then we can continue to create the embeddings using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6bfec8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "embeddings_model = HuggingFaceEmbeddings(show_progress=False) # Change to True for visual feedback\n",
    "embeddings = embeddings_model.embed_documents(sentences)\n",
    "\n",
    "print(len(embeddings), len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736ad8a2",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Ignore warnings about `tqdm` etc., nothing to do about it..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1fdf84",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "The \"unit\" that LangChain is working with is `Document`, so we'll first have to wrap all chunks/sentences in `Document`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9d7570",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [Document(page_content=sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f07e39",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Again, we'll have to bite the bullet and install LangcChain modules specific to an external service type (Qdrant database), another example of leaky abstractions:\n",
    "\n",
    "```\n",
    "jovyan@jupyter-user:~$ pip install langchain-qdrant\n",
    "```\n",
    "\n",
    "Then we can continue to create the embeddings using the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa338ab",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "LangcChain will attach a default id to each document as it is uploaded to qdrant, but we'll be providing integer ids (index of the sentence) to prevent that from happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9635fc6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "# https://python.langchain.com/v0.2/docs/integrations/vectorstores/qdrant/\n",
    "# https://api.python.langchain.com/en/latest/qdrant/langchain_qdrant.qdrant.QdrantVectorStore.html\n",
    "vector_store = QdrantVectorStore.from_documents(\n",
    "    documents,\n",
    "    embeddings_model,\n",
    "    url=\"http://10.129.20.4:6333\",\n",
    "    distance='Euclid',\n",
    "    collection_name=collection_name,\n",
    "    ids=list(range(len(documents))),\n",
    "    force_recreate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0a2b2",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "There are two things to note here, besides passing the documents (embeddings) and the requested ids, which is:\n",
    "\n",
    "1. Setting `force_recreate=True`, which is simply a convenience in a toy example like this, and\n",
    "2. using the string `'Euclid'` to define the distance metric used,\n",
    "\n",
    "The second point deserves some explanation:\n",
    "Using LangChain's own `EmbeddingDistance.EUCLIDEAN` (`from langchain.evaluation import EmbeddingDistance`) results in an error as it evaluates to the string `'euclidean'`, and the officially recommended solution is to import Qdrant's own definition using `from qdrant_client.models import Distance` and use `Distance.EUCLID` which evalutes to `'Euclid'`. Now, that could be the most blatant example of a _leaky abstraction_ that I've ever seen, and the world is a much better place if that wart is ignored and the literal `'Euclid'` is used instead.\n",
    "\n",
    "<!--\n",
    "# https://api.python.langchain.com/en/latest/qdrant/langchain_qdrant.qdrant.QdrantVectorStore.html#langchain_qdrant.qdrant.QdrantVectorStore\n",
    "# from langchain.evaluation import EmbeddingDistance\n",
    "# from qdrant_client.models import Distance, VectorParams # LEAKY ABSTRACTION (from official docs)\n",
    "\n",
    "# ValidationError: 1 validation error for VectorParams\n",
    "# distance\n",
    "#   Input should be 'Cosine', 'Euclid', 'Dot' or 'Manhattan' [type=enum, input_value=<EmbeddingDistance.EUCLIDEAN: 'euclidean'>, input_type=EmbeddingDistance]\n",
    "#     For further information visit https://errors.pydantic.dev/2.8/v/enum\n",
    "#\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5ce8c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div style=\"background-color:lightblue; padding:5px\">\n",
    "\n",
    "**Sidenote**: Using an existing collection\n",
    "\n",
    "To use an instance of `langchain_qdrant.Qdrant` on an _existing_ collection without loading any new documents or texts, you can use the `Qdrant.from_existing_collection()` method.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752c4ba1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Anyhow, now we can retrive chunks from the database using ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19cc85a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "vector_store.get_by_ids([5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f158aed8",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Let's do a quick sanity check like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0869fe1c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "query = sentences[5]\n",
    "print(query)\n",
    "embedded_query = embeddings_model.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293e57b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.similarity_search_with_score(\n",
    "    query=query, k=2\n",
    ")\n",
    "for doc, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3341560",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(((x-y)*(x-y) for x,y in zip(embedded_query, embedded_query)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612fb569",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_id6 = embeddings_model.embed_query(sentence[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e6a162",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(((x-y)*(x-y) for x,y in zip(embedded_query, embedded_id6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeaceb1",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Compute distance manually?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc39796f",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "In order to chain together the vector database with the LLM we need to configure a `retriever` object rather than just a database client (see e.g. <https://python.langchain.com/docs/how_to/vectorstore_retriever/>).\n",
    "\n",
    "The search_type is set to `\"similarity\"` (the default) and the number of requested hits are stated in a (kludgy) search kwargs blindly passed down to the underlying database client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e237f45",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1634f69c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## LangChain RAG\n",
    "\n",
    "Putting it together in a complete RAG example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4652529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an AI assistant. Your task is to understand the user question, and provide an answer.\n",
    "\n",
    "            Your answers are short, to the point, and written by an domain expert.\n",
    "            If you don't know the answer, simply state, \"I don't know\".\n",
    "\n",
    "            Use the following pieces of retrieved context to answer the question.\n",
    "\n",
    "            \n",
    "            {context}\n",
    "            \"\"\"\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1372038a",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Now we can pose a question to the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb1ad65",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "query = 'What is special about HackerNews?'\n",
    "result = rag_chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3fa6f4",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "The result returned has three parts; the query, the retrieved context, and the answer from the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193e3dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b0152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce3fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da388b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab576a",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Now, the above example does not live up to the LangChain name, so we could rewrite it in a \"LangChain-y\" style (see <https://python.langchain.com/docs/tutorials/rag/>). \n",
    "NB. Reuse of `retriever`, `prompt`, and `llm` from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b09895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_context(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_context, \"input\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser() # Can be left out in this example\n",
    ")\n",
    "\n",
    "answer = rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee42edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9266883c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "So, what benefits does LangChain bring to the table? I'm not sure. For an example such as this, I would say that it is of no use. Problem is that it is not difficult to find critical comments on its usefulness in large scale deployments (see e.g. [why we no longer use LangChain for building our AI agents](https://www.octomind.dev/blog/why-we-no-longer-use-langchain-for-building-our-ai-agents#:~:text=The%20problem%20with%20LangChain's%20abstractions,understand%20and%20frustrating%20to%20maintain.)). Maybe there is some middle ground where it is useful, I don't know. Probably depends on the use case. YMMV.\n",
    "\n",
    "At least I would not recommend starting out a new project based on LangChain from day one, but rather migrating to it after getting to understand the problem and making a proper cost/benefit analysis."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:percent,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
